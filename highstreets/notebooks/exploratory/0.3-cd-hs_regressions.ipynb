{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Ridge, HuberRegressor\n",
    "from sklearn import tree\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics.cluster import adjusted_rand_score\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn import metrics\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import (\n",
    "    silhouette_samples,\n",
    "    silhouette_score,\n",
    "    confusion_matrix,\n",
    "    ConfusionMatrixDisplay,\n",
    ")\n",
    "import matplotlib.cm as cm\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "\n",
    "import mlflow\n",
    "\n",
    "from highstreets.models import train_model as tm\n",
    "from highstreets.data import make_dataset as mhsd\n",
    "from highstreets.visualisation import visualise as vhsd\n",
    "from highstreets.features import build_features as bf\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv(find_dotenv())\n",
    "\n",
    "YOY_FILE = os.environ.get(\"YOY_FILE\")\n",
    "PROFILE_FILE = os.environ.get(\"PROFILE_FILE\")\n",
    "PROJECT_ROOT = os.environ.get(\"PROJECT_ROOT\")\n",
    "MLFLOW_TRACKING_URI = os.environ.get(\"MLFLOW_TRACKING_URI\")\n",
    "TC_LOOKUP = os.environ.get(\"TC_LOOKUP\")\n",
    "O2_CLUSTERS = os.environ.get(\"O2_CLUSTERS\")\n",
    "\n",
    "mlflow.set_tracking_uri(MLFLOW_TRACKING_URI)\n",
    "mlflow.set_experiment(\"High street profile regression experiments\")\n",
    "mlflow.sklearn.autolog(disable=True)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "sns.set_theme(style=\"darkgrid\")\n",
    "sns.set_context(\"notebook\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load mastercard spend data along with high street profiles and setup data arrays and time vectors for convenience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hsp = pd.read_excel(PROFILE_FILE)\n",
    "hsd_yoy = pd.read_csv(YOY_FILE, parse_dates=[\"week_start\"])\n",
    "\n",
    "# some important dates\n",
    "nb_dates = pd.to_datetime(\n",
    "    [\n",
    "        \"2020-03-24\",  # first lockdown starts\n",
    "        \"2020-06-15\",  # shops reopen\n",
    "        \"2020-11-05\",  # second lockdown starts\n",
    "        \"2020-12-02\",  # back to 'tier 2' (i.e. partial reopening)\n",
    "        \"2021-01-05\",  # third lockdown starts\n",
    "        \"2021-04-12\",  # shops reopen\n",
    "    ]\n",
    ")\n",
    "\n",
    "hsd_yoy_minimal = mhsd.stack_retail_we_wd(hsd_yoy, \"yoy_\")\n",
    "\n",
    "hsd_yoy_minimal.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates_2020 = (\"2020-04-15\", \"2020-10-31\")\n",
    "dates_2020_full = (\"2020-01-01\", \"2020-12-31\")\n",
    "dates_2021 = (\"2021-02-12\", \"2021-08-31\")\n",
    "dates_full = (\"2020-01-01\", \"2021-12-31\")\n",
    "\n",
    "data_2020 = mhsd.extract_data_array(hsd_yoy_minimal, dates_2020, \"txn_amt\")\n",
    "data_2021 = mhsd.extract_data_array(hsd_yoy_minimal, dates_2021, \"txn_amt\")\n",
    "data_2020_full = mhsd.extract_data_array(hsd_yoy_minimal, dates_2020_full, \"txn_amt\")\n",
    "data_full = mhsd.extract_data_array(hsd_yoy_minimal, dates_full, \"txn_amt\")\n",
    "\n",
    "start_times = {\"2020\": \"2020-04-01\", \"2021\": \"2021-04-12\", \"full\": \"2020-04-01\"}\n",
    "tvecs = {\"2020\": data_2020.index, \"2021\": data_2021.index, \"full\": data_full.index}\n",
    "arrays = {\n",
    "    \"2020\": np.transpose(data_2020.to_numpy()),\n",
    "    \"2021\": np.transpose(data_2021.to_numpy()),\n",
    "    \"full\": np.transpose(data_full.to_numpy()),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run k-means on 2020, 2021, and full data separately:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clus = 3\n",
    "max_iter = 50\n",
    "tol = 1e-2\n",
    "\n",
    "# 2020 data:\n",
    "kmeans20 = KMeans(\n",
    "    init=\"random\",\n",
    "    n_clusters=n_clus,\n",
    "    random_state=None,\n",
    "    max_iter=max_iter,\n",
    "    tol=tol,\n",
    "    copy_x=True,\n",
    "    verbose=0,\n",
    "    n_init=10,\n",
    ")\n",
    "kmeans20.fit(np.transpose(data_2020.to_numpy()))\n",
    "\n",
    "# 2021 data:\n",
    "kmeans21 = KMeans(\n",
    "    init=\"random\",\n",
    "    n_clusters=n_clus,\n",
    "    random_state=None,\n",
    "    max_iter=max_iter,\n",
    "    tol=tol,\n",
    "    copy_x=True,\n",
    "    verbose=0,\n",
    "    n_init=10,\n",
    ")\n",
    "kmeans21.fit(np.transpose(data_2021.to_numpy()))\n",
    "\n",
    "# full data:\n",
    "kmeansfull = KMeans(\n",
    "    init=\"random\",\n",
    "    n_clusters=n_clus,\n",
    "    random_state=None,\n",
    "    max_iter=max_iter,\n",
    "    tol=tol,\n",
    "    copy_x=True,\n",
    "    verbose=0,\n",
    "    n_init=10,\n",
    ")\n",
    "kmeansfull.fit(np.transpose(data_full.to_numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regressions: trying to summarise trends across time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unpooled regression - fit slope and intercept independently for each high street\n",
    "fit_lines = {}\n",
    "reg_model = {}\n",
    "\n",
    "reg_model[\"2020\"], fit_lines[\"2020\"] = bf.get_fit_lines(\n",
    "    start_times[\"2020\"], tvecs[\"2020\"], arrays[\"2020\"], robust=False\n",
    ")\n",
    "reg_model[\"2021\"], fit_lines[\"2021\"] = bf.get_fit_lines(\n",
    "    start_times[\"2021\"], tvecs[\"2021\"], arrays[\"2021\"], robust=False\n",
    ")\n",
    "reg_model[\"full\"], fit_lines[\"full\"] = bf.get_fit_lines(\n",
    "    start_times[\"full\"], tvecs[\"full\"], arrays[\"full\"], robust=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run K-means on fit parameters only for comparison with results of k-means run on the full timeseries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_cluster_20 = reg_model[\"2020\"].coef_\n",
    "y_cluster_20 = reg_model[\"2020\"].intercept_.reshape(-1, 1)\n",
    "# x_cluster_20 = array_2020.mean(1).reshape(-1,1)\n",
    "# y_cluster_20 = array_2020.mean(1).reshape(-1,1)\n",
    "\n",
    "fit_params_20 = np.concatenate((x_cluster_20, y_cluster_20), axis=1)\n",
    "\n",
    "n_clus = 3\n",
    "\n",
    "kmeans_lines = KMeans(\n",
    "    init=\"random\",\n",
    "    n_clusters=n_clus,\n",
    "    random_state=None,\n",
    "    max_iter=max_iter,\n",
    "    tol=tol,\n",
    "    copy_x=True,\n",
    "    verbose=0,\n",
    "    n_init=10,\n",
    ")\n",
    "kmeans_lines.fit(fit_params_20)\n",
    "\n",
    "x_cluster_21 = reg_model[\"2021\"].coef_\n",
    "y_cluster_21 = reg_model[\"2021\"].intercept_.reshape(-1, 1)\n",
    "# x_cluster_20 = array_2020.mean(1).reshape(-1,1)\n",
    "\n",
    "fit_params_21 = np.concatenate((x_cluster_21, y_cluster_21), axis=1)\n",
    "\n",
    "kmeans_lines_21 = KMeans(\n",
    "    init=\"random\",\n",
    "    n_clusters=n_clus,\n",
    "    random_state=None,\n",
    "    max_iter=max_iter,\n",
    "    tol=tol,\n",
    "    copy_x=True,\n",
    "    verbose=0,\n",
    "    n_init=10,\n",
    ")\n",
    "kmeans_lines_21.fit(fit_params_21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(14, 12), sharey=True)\n",
    "\n",
    "x_20 = x_cluster_20.flatten()\n",
    "y_20 = y_cluster_20.flatten()\n",
    "x_21 = x_cluster_21.flatten()\n",
    "y_21 = y_cluster_21.flatten()\n",
    "\n",
    "sns.scatterplot(x=x_20, y=y_20, hue=kmeans_lines.labels_, ax=axes[0][0])\n",
    "sns.scatterplot(x=x_20, y=y_20, hue=kmeans20.labels_, ax=axes[0][1])\n",
    "\n",
    "sns.scatterplot(x=x_21, y=y_21, hue=kmeans_lines_21.labels_, ax=axes[1][0])\n",
    "sns.scatterplot(x=x_21, y=y_21, hue=kmeans21.labels_, ax=axes[1][1])\n",
    "\n",
    "for ax in axes.reshape(-1):\n",
    "    yl = ax.get_ylim()\n",
    "    ax.plot([0, 0], yl, \"--r\")\n",
    "    ax.set_ylim([yl[0], 4])\n",
    "    ax.set_xlim([-0.01, 0.016])\n",
    "    ax.set_xlabel(\"Best fit slope\")\n",
    "    ax.set_ylabel(\"Intercept\")\n",
    "\n",
    "axes[0][0].set_title(\"Clustering run on slopes and intercepts\")\n",
    "axes[0][1].set_title(\"Clustering run on full timeseries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_shifts = fit_params_21 - fit_params_20\n",
    "sns.scatterplot(x=param_shifts[:, 0], y=param_shifts[:, 1], hue=kmeans_lines.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjusted_rand_score(kmeans20.labels_, kmeans21.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_diff = KMeans(\n",
    "    init=\"random\",\n",
    "    n_clusters=n_clus,\n",
    "    random_state=None,\n",
    "    max_iter=max_iter,\n",
    "    tol=tol,\n",
    "    copy_x=True,\n",
    "    verbose=0,\n",
    "    n_init=10,\n",
    ")\n",
    "kmeans_diff.fit(param_shifts)\n",
    "sns.scatterplot(x=param_shifts[:, 0], y=param_shifts[:, 1], hue=kmeans_diff.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_params_20 = np.concatenate((x_cluster_20, y_cluster_20), axis=1)\n",
    "fit_params_21 = np.concatenate((x_cluster_21, y_cluster_21), axis=1)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(fit_params_20)\n",
    "\n",
    "fit_params_scaled_20 = scaler.transform(fit_params_20)\n",
    "fit_params_scaled_21 = scaler.transform(fit_params_21)\n",
    "\n",
    "X = fit_params_20[:, 0]\n",
    "Y = fit_params_20[:, 1]\n",
    "\n",
    "param_shifts = fit_params_scaled_21 - fit_params_scaled_20\n",
    "\n",
    "U = param_shifts[:, 0]\n",
    "V = param_shifts[:, 1]\n",
    "\n",
    "ax, _ = plt.subplots(1, 1, figsize=(10, 8))\n",
    "plt.quiver(X, Y, U, V, [kmeans_lines.labels_])\n",
    "plt.gca().set_xlim([-0.0075, 0.0175])\n",
    "plt.gca().set_ylim([-1, 3])\n",
    "plt.set_cmap(\"Set1\")\n",
    "plt.ylabel(\"Intercept\")\n",
    "plt.xlabel(\"Slope\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot each highstreet with lines fit to each recovery period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vhsd.plot_all_profiles_full(\n",
    "#     {\"2020\": data_2020, \"2021\": data_2021, \"full\": data_full}, fit_lines\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sort highstreets by their 2020 mean and 2020 fit slope and plot by group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns by which we will sort the highstreets\n",
    "# (for example,\n",
    "# slope of the best fit line to 2020 recovery and the mean 2020)\n",
    "\n",
    "hit_percent_2020 = (\n",
    "    data_2020_full.loc[nb_dates[0] : nb_dates[1]].mean()\n",
    "    / data_2020_full.loc[\"2020-01-04\":\"2020-03-01\"].mean()\n",
    ").to_numpy()[:, np.newaxis]\n",
    "mean_2020 = (\n",
    "    data_2020_full.loc[nb_dates[0] : nb_dates[3], :].mean().to_numpy()[:, np.newaxis]\n",
    ")\n",
    "\n",
    "# sort_by = 'hit' #'mean'\n",
    "\n",
    "sort_cols = (\n",
    "    hit_percent_2020,\n",
    "    reg_model[\"2020\"].coef_,\n",
    ")\n",
    "\n",
    "plot_array = np.transpose(data_full.to_numpy())\n",
    "plot_tvec = data_full.index\n",
    "filename = \"2020-sorted-by-hit-slope.pdf\"\n",
    "\n",
    "vhsd.plot_highstreets_grouped(\n",
    "    plot_array,\n",
    "    plot_tvec,\n",
    "    sort_cols,\n",
    "    nb_dates,\n",
    "    filename,\n",
    "    xlim=(\"2020-01-01\", \"2020-12-31\"),\n",
    "    figure_title=\"2020\",\n",
    "    n_grp=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hit_percent_2020 = (\n",
    "    data_2020_full.loc[nb_dates[0] : nb_dates[1]].quantile(q=0.2, axis=0)\n",
    "    / data_2020_full.loc[\"2020-01-04\":\"2020-03-01\"].mean()\n",
    ").to_numpy()[:, np.newaxis]\n",
    "mean_2020 = (\n",
    "    data_2020_full.loc[nb_dates[0] : nb_dates[3], :].mean().to_numpy()[:, np.newaxis]\n",
    ")\n",
    "\n",
    "# sort_by = 'hit' #'mean'\n",
    "\n",
    "sort_cols = (\n",
    "    hit_percent_2020,\n",
    "    365\n",
    "    * reg_model[\n",
    "        \"2020\"\n",
    "    ].coef_,  # multiply by 365 to convert slopes into units of MRLI/year\n",
    ")\n",
    "\n",
    "ret = vhsd.plot_highstreets_grouped(\n",
    "    plot_array,\n",
    "    plot_tvec,\n",
    "    sort_cols,\n",
    "    nb_dates,\n",
    "    filename,\n",
    "    xlim=(\"2020-01-01\", \"2020-12-31\"),\n",
    "    figure_title=\"2020\",\n",
    "    n_grp=4,\n",
    "    equal_hs_per_bin=False,\n",
    "    low_pct=10,\n",
    "    high_pct=90,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sort highstreets by their 2021 mean and slope and plot in groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns by which we will sort the highstreets\n",
    "# (for example, slope of the best fit line to 2020 recovery\n",
    "# and the initial hit in 2020)\n",
    "sort_cols = (\n",
    "    data_2021.loc[\"2021-03-01\" : nb_dates[-1], :].mean().to_numpy()[:, np.newaxis],\n",
    "    reg_model[\"2021\"].coef_,\n",
    ")\n",
    "\n",
    "plot_array = np.transpose(data_full.to_numpy())\n",
    "plot_tvec = data_full.index\n",
    "filename = \"2021-sorted-by-mean-slope.pdf\"\n",
    "\n",
    "\n",
    "vhsd.plot_highstreets_grouped(\n",
    "    plot_array,\n",
    "    plot_tvec,\n",
    "    sort_cols,\n",
    "    nb_dates,\n",
    "    filename,\n",
    "    xlim=(\"2021-01-05\", \"2021-09-01\"),\n",
    "    figure_title=\"2021\",\n",
    "    n_grp=6,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sort highstreets by their 2020 means and slopes and plot across full period sorted into groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hs_id_name_lookup = dict(\n",
    "    zip(\n",
    "        data_2020_full.columns.get_level_values(1),\n",
    "        data_2020_full.columns.get_level_values(2),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns by which we will sort the highstreets\n",
    "# (for example, slope of the best fit line to 2020 recovery\n",
    "# and the initial hit in 2020)\n",
    "hit_percent_2021 = (\n",
    "    data_full.loc[\"2021-12-01\":\"2022-01-01\"].mean()\n",
    "    / data_full.loc[\"2021-08-01\":\"2021-10-01\"].mean()\n",
    ").to_numpy()[:, np.newaxis]\n",
    "\n",
    "mean_2020 = (\n",
    "    data_2020_full.loc[\"2020-03-14\":\"2020-11-01\", :].mean().to_numpy()[:, np.newaxis]\n",
    ")\n",
    "\n",
    "sort_cols = (\n",
    "    hit_percent_2020,\n",
    "    reg_model[\"2020\"].coef_,\n",
    ")\n",
    "\n",
    "plot_array = np.transpose(data_full.to_numpy())\n",
    "plot_tvec = data_full.index\n",
    "filename = \"full-sorted-by-2020-hit-slope.pdf\"\n",
    "\n",
    "vhsd.plot_highstreets_grouped(\n",
    "    plot_array,\n",
    "    plot_tvec,\n",
    "    sort_cols,\n",
    "    nb_dates,\n",
    "    filename,\n",
    "    xlim=(\"2020-01-01\", \"2021-09-01\"),\n",
    "    figure_title=\"Full period (sorted by 2020 params)\",\n",
    "    n_grp=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Append 2020 & 2021 means and fit lines to High Street Profiles for further analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = bf.append_profile_features(hsp, data_full, reg_model)\n",
    "stats = bf.clean_hs_profiles(stats)\n",
    "\n",
    "group_cols = [\"hit percent 2020\", \"slope 2020\"]\n",
    "low_pct, high_pct = 10, 90\n",
    "\n",
    "rcg_names = [\n",
    "    \"mrli_hp_2020_group\",\n",
    "    \"mrli_slope_2020_group\",\n",
    "    \"group\",\n",
    "]\n",
    "\n",
    "n_grp = 1\n",
    "\n",
    "stats = bf.hist2d_highstreets(\n",
    "    stats,\n",
    "    n_grp=n_grp,\n",
    "    group_cols=group_cols,\n",
    "    rcg_names=rcg_names,\n",
    "    low_pct=low_pct,\n",
    "    high_pct=high_pct,\n",
    ")\n",
    "\n",
    "stats_out_paul = stats[[\"highstreet_name\"] + rcg_names]\n",
    "\n",
    "stats_out_paul.to_csv(\n",
    "    f\"{PROJECT_ROOT}/data/HS_mrli_by_hitpct_slope_group_2020_{n_grp+2}.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histogram of numerical feature distributions to look for any further skewed features that should be transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_hist = stats.hist(figsize=(18, 15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at correlations between features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(30, 16))\n",
    "sns.set(font_scale=1.6)\n",
    "sns.heatmap(stats.corr(), annot=True, cmap=\"viridis\")\n",
    "plt.savefig(PROJECT_ROOT + \"/reports/figures/feature-correlations.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_features = [\n",
    "    \"mean 2020\",\n",
    "    \"mean 2021\",\n",
    "    \"hit percent 2020\",\n",
    "    \"hit percent 2021\",\n",
    "    \"percent_eating\",\n",
    "    \"percent_apparel\",\n",
    "    \"percent_retail\",\n",
    "    \"percent_we\",\n",
    "    \"percent_wd\",\n",
    "    \"slope 2020\",\n",
    "    \"slope 2021\",\n",
    "    \"num_addresses\",\n",
    "    \"pct residential addresses\",\n",
    "    \"average IMD2019 score\",\n",
    "    \"pct JSA 2021\",\n",
    "    \"pct work age\",\n",
    "    \"log_pct commercial addresses\",\n",
    "    \"log_Pop\",\n",
    "    \"log_2019 scale\",\n",
    "    \"log_pct offices\",\n",
    "    \"log_pct HW\",\n",
    "    \"log_pct employees\",\n",
    "    \"cluster_hourly\",\n",
    "    \"cluster_daily\",\n",
    "    \"cluster_size\",\n",
    "]\n",
    "\n",
    "X = stats[numerical_features].dropna().to_numpy()\n",
    "\n",
    "pipe = Pipeline(\n",
    "    [\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"pca\", PCA(n_components=10)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "pipe.fit(X)\n",
    "\n",
    "plt.plot(pipe.named_steps[\"pca\"].explained_variance_, marker=\"o\")\n",
    "\n",
    "X_red = pipe.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_profiles = KMeans(n_clusters=4).fit(X)\n",
    "\n",
    "metrics.silhouette_score(X, kmeans_profiles.labels_, metric=\"euclidean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "range_n_clusters = [2, 3, 4, 5]\n",
    "\n",
    "for n_clusters in range_n_clusters:\n",
    "    # Create a subplot with 1 row and 2 columns\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "    fig.set_size_inches(18, 7)\n",
    "\n",
    "    # The 1st subplot is the silhouette plot\n",
    "    # The silhouette coefficient can range from -1, 1 but in this example all\n",
    "    # lie within [-0.1, 1]\n",
    "    ax1.set_xlim([-0.1, 1])\n",
    "    # The (n_clusters+1)*10 is for inserting blank space between silhouette\n",
    "    # plots of individual clusters, to demarcate them clearly.\n",
    "    ax1.set_ylim([0, len(X) + (n_clusters + 1) * 10])\n",
    "\n",
    "    # Initialize the clusterer with n_clusters value and a random generator\n",
    "    # seed of 10 for reproducibility.\n",
    "    clusterer = KMeans(n_clusters=n_clusters, random_state=10)\n",
    "    cluster_labels = clusterer.fit_predict(X)\n",
    "\n",
    "    # The silhouette_score gives the average value for all the samples.\n",
    "    # This gives a perspective into the density and separation of the formed\n",
    "    # clusters\n",
    "    silhouette_avg = silhouette_score(X, cluster_labels)\n",
    "    print(\n",
    "        \"For n_clusters =\",\n",
    "        n_clusters,\n",
    "        \"The average silhouette_score is :\",\n",
    "        silhouette_avg,\n",
    "    )\n",
    "\n",
    "    # Compute the silhouette scores for each sample\n",
    "    sample_silhouette_values = silhouette_samples(X, cluster_labels)\n",
    "\n",
    "    y_lower = 10\n",
    "    for i in range(n_clusters):\n",
    "        # Aggregate the silhouette scores for samples belonging to\n",
    "        # cluster i, and sort them\n",
    "        ith_cluster_silhouette_values = sample_silhouette_values[cluster_labels == i]\n",
    "\n",
    "        ith_cluster_silhouette_values.sort()\n",
    "\n",
    "        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "        y_upper = y_lower + size_cluster_i\n",
    "\n",
    "        color = cm.nipy_spectral(float(i) / n_clusters)\n",
    "        ax1.fill_betweenx(\n",
    "            np.arange(y_lower, y_upper),\n",
    "            0,\n",
    "            ith_cluster_silhouette_values,\n",
    "            facecolor=color,\n",
    "            edgecolor=color,\n",
    "            alpha=0.7,\n",
    "        )\n",
    "\n",
    "        # Label the silhouette plots with their cluster numbers at the middle\n",
    "        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "\n",
    "        # Compute the new y_lower for next plot\n",
    "        y_lower = y_upper + 10  # 10 for the 0 samples\n",
    "\n",
    "    ax1.set_title(\"Silhouette plot for the various clusters.\")\n",
    "    ax1.set_xlabel(\"Silhouette coefficient values\")\n",
    "    ax1.set_ylabel(\"Cluster label\")\n",
    "\n",
    "    # The vertical line for average silhouette score of all the values\n",
    "    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
    "\n",
    "    ax1.set_yticks([])  # Clear the yaxis labels / ticks\n",
    "    ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
    "\n",
    "    # 2nd Plot showing the actual clusters formed\n",
    "    colors = cm.nipy_spectral(cluster_labels.astype(float) / n_clusters)\n",
    "    ax2.scatter(\n",
    "        X_red[:, 0],\n",
    "        X_red[:, 1],\n",
    "        marker=\".\",\n",
    "        s=30,\n",
    "        lw=0,\n",
    "        alpha=0.7,\n",
    "        c=colors,\n",
    "        edgecolor=\"k\",\n",
    "    )\n",
    "\n",
    "    # Labeling the clusters\n",
    "    centers = clusterer.cluster_centers_\n",
    "    centers = pipe.transform(centers)\n",
    "    # Draw white circles at cluster centers\n",
    "    ax2.scatter(\n",
    "        centers[:, 0],\n",
    "        centers[:, 1],\n",
    "        marker=\"o\",\n",
    "        c=\"white\",\n",
    "        alpha=1,\n",
    "        s=200,\n",
    "        edgecolor=\"k\",\n",
    "    )\n",
    "\n",
    "    for i, c in enumerate(centers):\n",
    "        ax2.scatter(c[0], c[1], marker=\"$%d$\" % i, alpha=1, s=50, edgecolor=\"k\")\n",
    "\n",
    "    ax2.set_title(\"High street clusters in PC projection\")\n",
    "    ax2.set_xlabel(\"1st PC\")\n",
    "    ax2.set_ylabel(\"2nd PC\")\n",
    "\n",
    "    plt.suptitle(\n",
    "        \"Silhouette analysis for KMeans clustering on sample data with n_clusters = %d\"\n",
    "        % n_clusters,\n",
    "        fontsize=14,\n",
    "        fontweight=\"bold\",\n",
    "    )\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scramble(a, axis=-1):\n",
    "    \"\"\"\n",
    "    Return an array with the values of `a` independently shuffled along the\n",
    "    given axis\n",
    "    \"\"\"\n",
    "    b = a.swapaxes(axis, -1)\n",
    "    n = a.shape[axis]\n",
    "    idx = np.random.choice(n, n, replace=False)\n",
    "    b = b[..., idx]\n",
    "    return b.swapaxes(axis, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = scramble(X, axis=0)\n",
    "\n",
    "kmeans_profiles_rand = KMeans(n_clusters=4).fit(X)\n",
    "\n",
    "range_n_clusters = [2, 3, 4, 5]\n",
    "\n",
    "for n_clusters in range_n_clusters:\n",
    "    # Create a subplot with 1 row and 2 columns\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "    fig.set_size_inches(18, 7)\n",
    "\n",
    "    # The 1st subplot is the silhouette plot\n",
    "    # The silhouette coefficient can range from -1, 1 but in this example all\n",
    "    # lie within [-0.1, 1]\n",
    "    ax1.set_xlim([-0.1, 1])\n",
    "    # The (n_clusters+1)*10 is for inserting blank space between silhouette\n",
    "    # plots of individual clusters, to demarcate them clearly.\n",
    "    ax1.set_ylim([0, len(X) + (n_clusters + 1) * 10])\n",
    "\n",
    "    # Initialize the clusterer with n_clusters value and a random generator\n",
    "    # seed of 10 for reproducibility.\n",
    "    clusterer = KMeans(n_clusters=n_clusters, random_state=12)\n",
    "    cluster_labels = clusterer.fit_predict(X)\n",
    "\n",
    "    # The silhouette_score gives the average value for all the samples.\n",
    "    # This gives a perspective into the density and separation of the formed\n",
    "    # clusters\n",
    "    silhouette_avg = silhouette_score(X, cluster_labels)\n",
    "    print(\n",
    "        \"For n_clusters =\",\n",
    "        n_clusters,\n",
    "        \"The average silhouette_score is :\",\n",
    "        silhouette_avg,\n",
    "    )\n",
    "\n",
    "    # Compute the silhouette scores for each sample\n",
    "    sample_silhouette_values = silhouette_samples(X, cluster_labels)\n",
    "\n",
    "    y_lower = 10\n",
    "    for i in range(n_clusters):\n",
    "        # Aggregate the silhouette scores for samples belonging to\n",
    "        # cluster i, and sort them\n",
    "        ith_cluster_silhouette_values = sample_silhouette_values[cluster_labels == i]\n",
    "\n",
    "        ith_cluster_silhouette_values.sort()\n",
    "\n",
    "        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "        y_upper = y_lower + size_cluster_i\n",
    "\n",
    "        color = cm.nipy_spectral(float(i) / n_clusters)\n",
    "        ax1.fill_betweenx(\n",
    "            np.arange(y_lower, y_upper),\n",
    "            0,\n",
    "            ith_cluster_silhouette_values,\n",
    "            facecolor=color,\n",
    "            edgecolor=color,\n",
    "            alpha=0.7,\n",
    "        )\n",
    "\n",
    "        # Label the silhouette plots with their cluster numbers at the middle\n",
    "        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "\n",
    "        # Compute the new y_lower for next plot\n",
    "        y_lower = y_upper + 10  # 10 for the 0 samples\n",
    "\n",
    "    ax1.set_title(\"Silhouette plot for the various clusters.\")\n",
    "    ax1.set_xlabel(\"Silhouette coefficient values\")\n",
    "    ax1.set_ylabel(\"Cluster label\")\n",
    "\n",
    "    # The vertical line for average silhouette score of all the values\n",
    "    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
    "\n",
    "    ax1.set_yticks([])  # Clear the yaxis labels / ticks\n",
    "    ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
    "\n",
    "    # 2nd Plot showing the actual clusters formed\n",
    "    colors = cm.nipy_spectral(cluster_labels.astype(float) / n_clusters)\n",
    "    ax2.scatter(\n",
    "        X_red[:, 0],\n",
    "        X_red[:, 1],\n",
    "        marker=\".\",\n",
    "        s=30,\n",
    "        lw=0,\n",
    "        alpha=0.7,\n",
    "        c=colors,\n",
    "        edgecolor=\"k\",\n",
    "    )\n",
    "\n",
    "    # Labeling the clusters\n",
    "    centers = clusterer.cluster_centers_\n",
    "    centers = pipe.transform(centers)\n",
    "    # Draw white circles at cluster centers\n",
    "    ax2.scatter(\n",
    "        centers[:, 0],\n",
    "        centers[:, 1],\n",
    "        marker=\"o\",\n",
    "        c=\"white\",\n",
    "        alpha=1,\n",
    "        s=200,\n",
    "        edgecolor=\"k\",\n",
    "    )\n",
    "\n",
    "    for i, c in enumerate(centers):\n",
    "        ax2.scatter(c[0], c[1], marker=\"$%d$\" % i, alpha=1, s=50, edgecolor=\"k\")\n",
    "\n",
    "    ax2.set_title(\"High street clusters in PC projection\")\n",
    "    ax2.set_xlabel(\"1st PC\")\n",
    "    ax2.set_ylabel(\"2nd PC\")\n",
    "\n",
    "    plt.suptitle(\n",
    "        \"Silhouette analysis for KMeans clustering on sample data with n_clusters = %d\"\n",
    "        % n_clusters,\n",
    "        fontsize=14,\n",
    "        fontweight=\"bold\",\n",
    "    )\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit some simple regressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = [\n",
    "    \"percent_eating\",\n",
    "    \"percent_apparel\",\n",
    "    \"percent_retail\",\n",
    "    \"percent_we\",\n",
    "    \"percent_wd\",\n",
    "    \"num_addresses\",\n",
    "    \"pct residential addresses\",\n",
    "    \"average IMD2019 score\",\n",
    "    \"loac rank 1\",\n",
    "    \"ptal\",\n",
    "    \"pct JSA 2021\",\n",
    "    \"pct work age\",\n",
    "    \"log_pct commercial addresses\",\n",
    "    \"log_Pop\",\n",
    "    \"log_2019 scale\",\n",
    "    \"log_pct offices\",\n",
    "    \"log_pct HW\",\n",
    "    \"log_pct employees\",\n",
    "    \"cluster_daily\",\n",
    "    \"cluster_hourly\",\n",
    "    \"cluster_size\",\n",
    "]\n",
    "\n",
    "target_col = [\"slope 2020\"]\n",
    "\n",
    "# drop non-feature columns\n",
    "data = stats[feature_cols + target_col]\n",
    "\n",
    "# One-hot encode categorical features\n",
    "data = pd.get_dummies(data).dropna()\n",
    "\n",
    "# define target and features\n",
    "y = data[target_col]\n",
    "X = data.drop(columns=target_col)\n",
    "\n",
    "# make train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline model is ridge regression with regularization parameter chosen with cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with mlflow.start_run() as run:\n",
    "\n",
    "    ridge_model = Ridge()\n",
    "    tuned_params = {\"model__alpha\": [200, 300, 400, 500, 600, 700, 800, 900]}\n",
    "    best_rrm, fig_train_test = tm.run_experiment_w_cv(\n",
    "        ridge_model, tuned_params, X_train, X_test, y_train, y_test\n",
    "    )\n",
    "\n",
    "    mlflow.log_metrics(tm.evaluate(best_rrm, X_test, y_test))\n",
    "    mlflow.log_param(\"regression target\", target_col)\n",
    "    mlflow.sklearn.log_model(best_rrm, \"Ridge Regression Model\")\n",
    "    mlflow.log_figure(fig_train_test, \"train_test_results.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Huber regression to see if robustness to outliers in the target variable helps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with mlflow.start_run():\n",
    "    huber_model = HuberRegressor()\n",
    "\n",
    "    tuned_params = {\n",
    "        \"model__epsilon\": [1.35],\n",
    "        \"model__alpha\": [300, 400, 600],\n",
    "    }\n",
    "\n",
    "    best_huber, fig = tm.run_experiment_w_cv(\n",
    "        huber_model, tuned_params, X_train, X_test, y_train, y_test\n",
    "    )\n",
    "\n",
    "    mlflow.log_metrics(tm.evaluate(best_huber, X_test, y_test))\n",
    "    mlflow.log_param(\"regression target\", target_col)\n",
    "    mlflow.sklearn.log_model(best_huber, \"Huber Regression Model\")\n",
    "    mlflow.log_figure(fig, \"train_test_results.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next try Support Vector Regression with hyperparameters chosen by cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with mlflow.start_run():\n",
    "\n",
    "    tuned_parameters = {\n",
    "        \"model__kernel\": [\"rbf\", \"poly\"],\n",
    "        \"model__C\": [0.0001, 0.001, 0.01, 0.1, 0.2],\n",
    "        \"model__epsilon\": [0.0009, 0.001, 0.0015, 0.002],\n",
    "    }\n",
    "\n",
    "    svr_model = SVR(gamma=\"scale\")\n",
    "\n",
    "    best_svr, fig = tm.run_experiment_w_cv(\n",
    "        svr_model, tuned_parameters, X_train, X_test, y_train, y_test\n",
    "    )\n",
    "\n",
    "    mlflow.log_metrics(tm.evaluate(best_svr, X_test, y_test))\n",
    "    mlflow.log_param(\"regression target\", target_col)\n",
    "    mlflow.sklearn.log_model(best_svr, \"SVR Model\")\n",
    "    mlflow.log_figure(fig, \"train_test_results.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision tree regressors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with mlflow.start_run():\n",
    "\n",
    "    tuned_parameters = {\n",
    "        \"model__max_depth\": [2, 4, 6, 8],\n",
    "        \"model__criterion\": [\"squared_error\", \"friedman_mse\", \"absolute_error\"],\n",
    "    }\n",
    "\n",
    "    dtr_model = tree.DecisionTreeRegressor()\n",
    "\n",
    "    best_dtr, fig = tm.run_experiment_w_cv(\n",
    "        dtr_model, tuned_parameters, X_train, X_test, y_train, y_test\n",
    "    )\n",
    "\n",
    "    mlflow.log_metrics(tm.evaluate(best_dtr, X_test, y_test))\n",
    "    mlflow.log_param(\"regression target\", target_col)\n",
    "    mlflow.sklearn.log_model(best_dtr, \"Decision tree regressor Model\")\n",
    "    mlflow.log_figure(fig, \"train_test_results.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here we try sorting and grouping highstreets by 2020 mean and slope and seeing if these groupings can be predicted from profile data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(stats[\"mrli_hp_2020_group\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode categorical features\n",
    "stats_target = stats.groupby(\"mrli_hp_2020_group\").get_group(0)\n",
    "\n",
    "data = pd.get_dummies(stats_target.drop(columns=\"highstreet_name\")).dropna()\n",
    "\n",
    "target_variables = [\n",
    "    \"mean 2020\",\n",
    "    \"mean 2021\",\n",
    "    \"slope 2020\",\n",
    "    \"slope 2021\",\n",
    "]\n",
    "\n",
    "target_col = \"slope 2020\"\n",
    "\n",
    "drop_cols = [\n",
    "    \"mean_slope_group\",\n",
    "    \"mean_group\",\n",
    "    \"slope_group\",\n",
    "    \"mrli_yoy_mean_2020_recovery\",\n",
    "    \"mrli_fit_slope_2020_recovery\",\n",
    "]\n",
    "\n",
    "y = data[target_col]\n",
    "X = data.drop(columns=target_variables + drop_cols)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_parameters = {\n",
    "    \"model__kernel\": [\"rbf\", \"poly\"],\n",
    "    \"model__C\": [0.001, 0.01, 0.1, 0.2],\n",
    "    \"model__epsilon\": [0.0009, 0.001, 0.0015, 0.002],\n",
    "}\n",
    "\n",
    "svr_model = SVR(gamma=\"scale\")\n",
    "\n",
    "best_svr = tm.run_experiment_w_cv(\n",
    "    svr_model, tuned_parameters, X_train, X_test, y_train, y_test\n",
    ")\n",
    "\n",
    "svr_r2, svr_mae, svr_mse = tm.evaluate(best_svr, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try classification with hit/slope group as target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = stats.dropna()\n",
    "\n",
    "non_feature_cols = [\n",
    "    \"mean 2020\",\n",
    "    \"mean 2021\",\n",
    "    \"slope 2021\",\n",
    "    \"hit percent 2020\",\n",
    "    \"hit percent 2021\",\n",
    "    \"highstreet_name\",\n",
    "] + rcg_names\n",
    "\n",
    "target_col = rcg_names[2]\n",
    "\n",
    "y = stats[target_col]\n",
    "\n",
    "le = LabelEncoder()\n",
    "le.fit(y)\n",
    "y_enc = le.transform(y)\n",
    "\n",
    "# drop non-feature columns\n",
    "X = stats.drop(columns=non_feature_cols)\n",
    "\n",
    "# One-hot encode categorical features\n",
    "X = pd.get_dummies(X)\n",
    "\n",
    "# make train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_enc, test_size=0.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb_classifier = GaussianNB()\n",
    "\n",
    "gnb_classifier.fit(X_train, y_train)\n",
    "\n",
    "y_pred = gnb_classifier.predict(X_test)\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "\n",
    "disp.plot()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9033d2e574670c4f95674482153ae32585408e2977a1def4923e0a03b8406a5f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('prd_replication_env': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
