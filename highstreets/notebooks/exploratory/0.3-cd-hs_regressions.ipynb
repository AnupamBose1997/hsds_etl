{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Ridge, HuberRegressor\n",
    "from sklearn import tree\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "import mlflow\n",
    "\n",
    "import missingno as msno\n",
    "\n",
    "from highstreets.models import train_model as tm\n",
    "from highstreets.data import make_dataset as mhsd\n",
    "from highstreets.visualisation import visualise as vhsd\n",
    "from highstreets.features import build_features as bf\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv(find_dotenv())\n",
    "\n",
    "YOY_FILE = os.environ.get(\"YOY_FILE\")\n",
    "PROFILE_FILE = os.environ.get(\"PROFILE_FILE\")\n",
    "PROJECT_ROOT = os.environ.get(\"PROJECT_ROOT\")\n",
    "MLFLOW_TRACKING_URI = os.environ.get(\"MLFLOW_TRACKING_URI\")\n",
    "\n",
    "mlflow.set_tracking_uri(MLFLOW_TRACKING_URI)\n",
    "mlflow.set_experiment(\"High street profile regression experiments\")\n",
    "mlflow.sklearn.autolog(disable=True)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "sns.set_theme(style=\"darkgrid\")\n",
    "sns.set_context(\"notebook\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load mastercard spend data along with high street profiles and setup data arrays and time vectors for convenience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hsp = pd.read_excel(PROFILE_FILE)\n",
    "hsd_yoy = pd.read_csv(YOY_FILE, parse_dates=[\"week_start\"])\n",
    "\n",
    "# some important dates\n",
    "nb_dates = pd.to_datetime(\n",
    "    [\n",
    "        \"2020-03-24\",  # first lockdown starts\n",
    "        \"2020-06-15\",  # shops reopen\n",
    "        \"2020-11-05\",  # second lockdown starts\n",
    "        \"2020-12-02\",  # back to 'tier 2' (i.e. partial reopening)\n",
    "        \"2021-01-05\",  # third lockdown starts\n",
    "        \"2021-04-12\",  # shops reopen\n",
    "    ]\n",
    ")\n",
    "\n",
    "# average weekday and weekend expenditure (should probably relax this\n",
    "# later - no need to lose information)\n",
    "hsd_yoy_minimal = mhsd.stack_retail_we_wd(hsd_yoy, \"yoy_\")\n",
    "\n",
    "dates_2020 = (\"2020-04-15\", \"2020-10-31\")\n",
    "dates_2020_full = (\"2020-01-01\", \"2020-12-31\")\n",
    "dates_2021 = (\"2021-02-12\", \"2021-08-31\")\n",
    "dates_full = (\"2020-01-01\", \"2021-12-31\")\n",
    "\n",
    "data_2020 = mhsd.extract_data_array(hsd_yoy_minimal, dates_2020, \"txn_amt\")\n",
    "data_2021 = mhsd.extract_data_array(hsd_yoy_minimal, dates_2021, \"txn_amt\")\n",
    "data_2020_full = mhsd.extract_data_array(hsd_yoy_minimal, dates_2020_full, \"txn_amt\")\n",
    "data_full = mhsd.extract_data_array(hsd_yoy_minimal, dates_full, \"txn_amt\")\n",
    "\n",
    "start_times = {\"2020\": \"2020-04-01\", \"2021\": \"2021-04-12\", \"full\": \"2020-04-01\"}\n",
    "tvecs = {\"2020\": data_2020.index, \"2021\": data_2021.index, \"full\": data_full.index}\n",
    "arrays = {\n",
    "    \"2020\": np.transpose(data_2020.to_numpy()),\n",
    "    \"2021\": np.transpose(data_2021.to_numpy()),\n",
    "    \"full\": np.transpose(data_full.to_numpy()),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run k-means on 2020, 2021, and full data separately:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clus = 3\n",
    "max_iter = 50\n",
    "tol = 1e-2\n",
    "\n",
    "# 2020 data:\n",
    "kmeans20 = KMeans(\n",
    "    init=\"random\",\n",
    "    n_clusters=n_clus,\n",
    "    random_state=None,\n",
    "    max_iter=max_iter,\n",
    "    tol=tol,\n",
    "    copy_x=True,\n",
    "    verbose=0,\n",
    "    n_init=10,\n",
    ")\n",
    "kmeans20.fit(np.transpose(data_2020.to_numpy()))\n",
    "\n",
    "# 2021 data:\n",
    "kmeans21 = KMeans(\n",
    "    init=\"random\",\n",
    "    n_clusters=n_clus,\n",
    "    random_state=None,\n",
    "    max_iter=max_iter,\n",
    "    tol=tol,\n",
    "    copy_x=True,\n",
    "    verbose=0,\n",
    "    n_init=10,\n",
    ")\n",
    "kmeans21.fit(np.transpose(data_2021.to_numpy()))\n",
    "\n",
    "# full data:\n",
    "kmeansfull = KMeans(\n",
    "    init=\"random\",\n",
    "    n_clusters=n_clus,\n",
    "    random_state=None,\n",
    "    max_iter=max_iter,\n",
    "    tol=tol,\n",
    "    copy_x=True,\n",
    "    verbose=0,\n",
    "    n_init=10,\n",
    ")\n",
    "kmeansfull.fit(np.transpose(data_full.to_numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regressions: trying to summarise trends across time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unpooled regression - fit slope and intercept independently for each high street\n",
    "fit_lines = {}\n",
    "reg_model = {}\n",
    "\n",
    "reg_model[\"2020\"], fit_lines[\"2020\"] = mhsd.get_fit_lines(\n",
    "    start_times[\"2020\"], tvecs[\"2020\"], arrays[\"2020\"], robust=False\n",
    ")\n",
    "reg_model[\"2021\"], fit_lines[\"2021\"] = mhsd.get_fit_lines(\n",
    "    start_times[\"2021\"], tvecs[\"2021\"], arrays[\"2021\"], robust=False\n",
    ")\n",
    "reg_model[\"full\"], fit_lines[\"full\"] = mhsd.get_fit_lines(\n",
    "    start_times[\"full\"], tvecs[\"full\"], arrays[\"full\"], robust=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot each highstreet with lines fit to each recovery period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vhsd.plot_all_profiles_full(\n",
    "#     {\"2020\": data_2020, \"2021\": data_2021, \"full\": data_full}, fit_lines\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sort highstreets by their 2020 mean and 2020 fit slope and plot by group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns by which we will sort the highstreets\n",
    "# (for example,\n",
    "# slope of the best fit line to 2020 recovery and the mean 2020)\n",
    "sort_cols = (\n",
    "    data_2020_full.loc[nb_dates[0] : nb_dates[3], :].mean().to_numpy(),\n",
    "    reg_model[\"2020\"].coef_,\n",
    ")\n",
    "\n",
    "plot_array = np.transpose(data_full.to_numpy())\n",
    "plot_tvec = data_full.index\n",
    "filename = \"2020-sorted-by-mean-slope.pdf\"\n",
    "\n",
    "vhsd.plot_highstreets_grouped(\n",
    "    plot_array,\n",
    "    plot_tvec,\n",
    "    sort_cols,\n",
    "    nb_dates,\n",
    "    filename,\n",
    "    xlim=(\"2020-01-01\", \"2020-12-31\"),\n",
    "    figure_title=\"2020\",\n",
    "    n_grp=6,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sort highstreets by their 2021 mean and slope and plot in groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns by which we will sort the highstreets\n",
    "# (for example, slope of the best fit line to 2020 recovery\n",
    "# and the initial hit in 2020)\n",
    "sort_cols = (\n",
    "    data_2021.loc[\"2021-03-01\" : nb_dates[-1], :].mean().to_numpy(),\n",
    "    reg_model[\"2021\"].coef_,\n",
    ")\n",
    "\n",
    "plot_array = np.transpose(data_full.to_numpy())\n",
    "plot_tvec = data_full.index\n",
    "filename = \"2021-sorted-by-mean-slope.pdf\"\n",
    "\n",
    "vhsd.plot_highstreets_grouped(\n",
    "    plot_array,\n",
    "    plot_tvec,\n",
    "    sort_cols,\n",
    "    nb_dates,\n",
    "    filename,\n",
    "    xlim=(\"2021-03-01\", \"2021-09-01\"),\n",
    "    figure_title=\"2021\",\n",
    "    n_grp=6,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sort highstreets by their 2020 means and slopes and plot across full period sorted into groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns by which we will sort the highstreets\n",
    "# (for example, slope of the best fit line to 2020 recovery\n",
    "# and the initial hit in 2020)\n",
    "sort_cols = (\n",
    "    data_2020_full.loc[\"2020-03-14\":\"2020-11-01\", :].mean().to_numpy(),\n",
    "    reg_model[\"2020\"].coef_,\n",
    ")\n",
    "\n",
    "plot_array = np.transpose(data_full.to_numpy())\n",
    "plot_tvec = data_full.index\n",
    "filename = \"full-sorted-by-2020-mean-slope.pdf\"\n",
    "\n",
    "vhsd.plot_highstreets_grouped(\n",
    "    plot_array,\n",
    "    plot_tvec,\n",
    "    sort_cols,\n",
    "    nb_dates,\n",
    "    filename,\n",
    "    xlim=(\"2020-01-01\", \"2021-09-01\"),\n",
    "    figure_title=\"Full period (sorted by 2020 params)\",\n",
    "    n_grp=6,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Append 2020 & 2021 means and fit lines to High Street Profiles for further analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = mhsd.append_profile_features(hsp, data_full, reg_model)\n",
    "stats = bf.clean_hs_profiles(stats)\n",
    "stats.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msno.matrix(stats);  # Option 3: With missingno"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at correlations between features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(30, 16))\n",
    "sns.heatmap(stats.corr(), annot=True, cmap=\"viridis\")\n",
    "plt.savefig(PROJECT_ROOT + \"/reports/figures/feature-correlations.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit some simple regressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode categorical features\n",
    "data = pd.get_dummies(stats)\n",
    "\n",
    "target_variables = [\n",
    "    \"mean 2020\",\n",
    "    \"mean 2021\",\n",
    "    \"slope 2020\",\n",
    "    \"slope 2021\",\n",
    "]\n",
    "\n",
    "target_col = \"mean 2020\"\n",
    "\n",
    "y = data[target_col]\n",
    "X = data.drop(columns=target_variables)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline model is ridge regression with regularization parameter chosen with cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with mlflow.start_run():\n",
    "\n",
    "    ridge_model = Ridge()\n",
    "    tuned_params = {\"model__alpha\": [200, 300, 400, 500, 600, 700, 800, 900]}\n",
    "    best_rrm, fig_train_test, _ = tm.run_experiment_w_cv(\n",
    "        ridge_model, tuned_params, X_train, X_test, y_train, y_test\n",
    "    )\n",
    "\n",
    "    mlflow.log_metrics(tm.evaluate(best_rrm, X_test, y_test))\n",
    "    mlflow.log_param(\"regression target\", target_col)\n",
    "    mlflow.sklearn.log_model(best_rrm, \"Ridge Regression Model\")\n",
    "    mlflow.log_figure(fig_train_test, \"train_test_results.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Huber regression to see if robustness to outliers in the target variable helps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with mlflow.start_run():\n",
    "    huber_model = HuberRegressor()\n",
    "\n",
    "    tuned_params = {\n",
    "        \"model__epsilon\": [1.35],\n",
    "        \"model__alpha\": [300, 400, 600],\n",
    "    }\n",
    "\n",
    "    best_huber, fig, ax = tm.run_experiment_w_cv(\n",
    "        huber_model, tuned_params, X_train, X_test, y_train, y_test\n",
    "    )\n",
    "\n",
    "    mlflow.log_metrics(tm.evaluate(best_huber, X_test, y_test))\n",
    "    mlflow.log_param(\"regression target\", target_col)\n",
    "    mlflow.sklearn.log_model(best_huber, \"Huber Regression Model\")\n",
    "    mlflow.log_figure(fig, \"train_test_results.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next try Support Vector Regression with hyperparameters chosen by cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_parameters = {\n",
    "    \"model__kernel\": [\"rbf\", \"poly\"],\n",
    "    \"model__C\": [0.001, 0.01, 0.1, 0.2],\n",
    "    \"model__epsilon\": [0.0009, 0.001, 0.0015, 0.002],\n",
    "}\n",
    "\n",
    "svr_model = SVR(gamma=\"scale\")\n",
    "\n",
    "best_svr = tm.run_experiment_w_cv(\n",
    "    svr_model, tuned_parameters, X_train, X_test, y_train, y_test\n",
    ")\n",
    "\n",
    "svr_r2, svr_mae, svr_mse = tm.evaluate(best_svr, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision tree regressors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_parameters = {\n",
    "    \"model__max_depth\": [2, 4, 6, 8],\n",
    "    \"model__criterion\": [\"squared_error\", \"friedman_mse\", \"absolute_error\", \"poisson\"],\n",
    "}\n",
    "\n",
    "dtr_model = tree.DecisionTreeRegressor()\n",
    "\n",
    "best_dtr = tm.run_experiment_w_cv(\n",
    "    dtr_model, tuned_parameters, X_train, X_test, y_train, y_test\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here we try sorting and grouping highstreets by 2020 mean and slope and seeing if these groupings can be predicted from profile data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_w_grps = bf.add_split_group_vals(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_w_grps.groupby([\"group_mean 2020\"]).groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9033d2e574670c4f95674482153ae32585408e2977a1def4923e0a03b8406a5f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('prd_replication_env': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
