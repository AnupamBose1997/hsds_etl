{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# standard library\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from highstreets.data import make_dataset as mhsd\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "import numpyro\n",
    "from numpyro.infer import MCMC, NUTS, Predictive, init_to_feasible\n",
    "from numpyro.infer.reparam import TransformReparam\n",
    "import numpyro.distributions as dist\n",
    "from jax import random\n",
    "import arviz as az\n",
    "\n",
    "import dill\n",
    "\n",
    "assert numpyro.__version__.startswith(\"0.9.1\")\n",
    "\n",
    "load_dotenv(find_dotenv())\n",
    "\n",
    "YOY_FILE = os.environ.get(\"YOY_FILE\")\n",
    "PROFILE_FILE = os.environ.get(\"PROFILE_FILE\")\n",
    "PROJECT_ROOT = os.environ.get(\"PROJECT_ROOT\")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "sns.set_theme(style=\"darkgrid\")\n",
    "sns.set_context(\"notebook\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load mastercard spend data along with high street profiles and setup data arrays and time vectors for convenience\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "hsp = pd.read_excel(PROFILE_FILE)\n",
    "hsd_yoy = pd.read_csv(YOY_FILE, parse_dates=[\"week_start\"])\n",
    "\n",
    "# some important dates\n",
    "nb_dates = pd.to_datetime(\n",
    "    [\n",
    "        \"2020-03-24\",  # first lockdown starts\n",
    "        \"2020-06-15\",  # shops reopen\n",
    "        \"2020-11-05\",  # second lockdown starts\n",
    "        \"2020-12-02\",  # back to 'tier 2' (i.e. partial reopening)\n",
    "        \"2021-01-05\",  # third lockdown starts\n",
    "        \"2021-04-12\",  # shops reopen\n",
    "    ]\n",
    ")\n",
    "\n",
    "# average weekday and weekend expenditure (should probably relax this\n",
    "# later - no need to lose information)\n",
    "hsd_yoy_minimal = mhsd.avg_retail_wd_we(hsd_yoy, \"yoy_\").dropna(how=\"any\", axis=\"rows\")\n",
    "\n",
    "dates_2020 = (\"2020-04-15\", \"2020-10-31\")\n",
    "dates_2020_full = (\"2020-01-01\", \"2020-12-31\")\n",
    "dates_2021 = (\"2021-02-12\", \"2021-08-31\")\n",
    "dates_full = (\"2020-01-01\", \"2021-12-31\")\n",
    "\n",
    "data_2020 = mhsd.extract_data_array(hsd_yoy_minimal, dates_2020, \"txn_amt\")\n",
    "data_2021 = mhsd.extract_data_array(hsd_yoy_minimal, dates_2021, \"txn_amt\")\n",
    "data_2020_full = mhsd.extract_data_array(hsd_yoy_minimal, dates_2020_full, \"txn_amt\")\n",
    "data_full = mhsd.extract_data_array(hsd_yoy_minimal, dates_full, \"txn_amt\")\n",
    "\n",
    "start_times = {\"2020\": \"2020-04-01\", \"2021\": \"2021-04-12\", \"full\": \"2020-04-01\"}\n",
    "tvecs = {\"2020\": data_2020.index, \"2021\": data_2021.index, \"full\": data_full.index}\n",
    "arrays = {\n",
    "    \"2020\": np.transpose(data_2020.to_numpy()),\n",
    "    \"2021\": np.transpose(data_2021.to_numpy()),\n",
    "    \"full\": np.transpose(data_full.to_numpy()),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up data to be used in hierarcical regressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictors = [\n",
    "    \"percentage of commercial addresses (%)\",\n",
    "    \"total estimated number of home workers\",\n",
    "    \"Sum_y2019_07wd\",\n",
    "]\n",
    "full_data = (\n",
    "    hsd_yoy_minimal.join(\n",
    "        hsp[[\"highstreet_id\"] + predictors],\n",
    "        on=\"highstreet_id\",\n",
    "        how=\"left\",\n",
    "        lsuffix=\"_left\",\n",
    "        rsuffix=\"_right\",\n",
    "    )\n",
    "    .drop([\"highstreet_id_right\", \"txn_cnt\"], axis=1)\n",
    "    .rename(columns={\"highstreet_id_left\": \"highstreet_id\"})\n",
    ")\n",
    "full_data[\"weeks_since_start\"] = (\n",
    "    full_data.index - pd.to_datetime(nb_dates[0])\n",
    ") / pd.Timedelta(1, \"W\")\n",
    "train = full_data.loc[nb_dates[0] : nb_dates[1]]\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define hierarchical regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(highstreet_id, weeks, hs_obs=None):\n",
    "    mu_a = numpyro.sample(\"mu_a\", dist.Normal(0.0, 1.0))\n",
    "    sigma_a = numpyro.sample(\"sigma_a\", dist.HalfNormal(1.0))\n",
    "    mu_b = numpyro.sample(\"mu_b\", dist.Normal(0.0, 1.0))\n",
    "    sigma_b = numpyro.sample(\"sigma_b\", dist.HalfNormal(1.0))\n",
    "\n",
    "    unique_hs_ids = np.unique(highstreet_id)\n",
    "    n_hs = len(unique_hs_ids)\n",
    "\n",
    "    with numpyro.plate(\"plate_i\", n_hs):\n",
    "        a = numpyro.sample(\"a\", dist.Normal(mu_a, sigma_a))\n",
    "        b = numpyro.sample(\"b\", dist.Normal(mu_b, sigma_b))\n",
    "\n",
    "    sigma = numpyro.sample(\"sigma\", dist.HalfNormal(1.0))\n",
    "    hs_est = a[highstreet_id] + b[highstreet_id] * weeks\n",
    "\n",
    "    with numpyro.plate(\"data\", len(highstreet_id)):\n",
    "        numpyro.sample(\"obs\", dist.Normal(hs_est, sigma), obs=hs_obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define non-centered version of the same model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_noncentered(highstreet_id, weeks, hs_obs=None):\n",
    "    mu_a = numpyro.sample(\"mu_a\", dist.Normal(0.0, 1.0))\n",
    "    sigma_a = numpyro.sample(\"sigma_a\", dist.HalfNormal(1.0))\n",
    "    mu_b = numpyro.sample(\"mu_b\", dist.Normal(0.0, 1.0))\n",
    "    sigma_b = numpyro.sample(\"sigma_b\", dist.HalfNormal(1.0))\n",
    "\n",
    "    unique_hs_ids = np.unique(highstreet_id)\n",
    "    n_hs = len(unique_hs_ids)\n",
    "\n",
    "    with numpyro.plate(\"plate_i\", n_hs):\n",
    "        with numpyro.handlers.reparam(\n",
    "            config={\n",
    "                \"a\": TransformReparam(),\n",
    "                \"b\": TransformReparam(),\n",
    "            }\n",
    "        ):\n",
    "            a = numpyro.sample(\n",
    "                \"a\",\n",
    "                dist.TransformedDistribution(\n",
    "                    dist.Normal(0.0, 1.0),\n",
    "                    dist.transforms.AffineTransform(mu_a, sigma_a),\n",
    "                ),\n",
    "            )\n",
    "            b = numpyro.sample(\n",
    "                \"b\",\n",
    "                dist.TransformedDistribution(\n",
    "                    dist.Normal(0.0, 1.0),\n",
    "                    dist.transforms.AffineTransform(mu_b, sigma_b),\n",
    "                ),\n",
    "            )\n",
    "\n",
    "    sigma = numpyro.sample(\"sigma\", dist.HalfNormal(1.0))\n",
    "    hs_est = a[highstreet_id] + b[highstreet_id] * weeks\n",
    "\n",
    "    with numpyro.plate(\"data\", len(highstreet_id)):\n",
    "        numpyro.sample(\"obs\", dist.Normal(hs_est, sigma), obs=hs_obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "hs_obs = train[\"txn_amt\"].values\n",
    "weeks = train[\"weeks_since_start\"].values\n",
    "highstreet_id = train[\"highstreet_id\"].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample from the posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nuts_kernel = NUTS(model, init_strategy=init_to_feasible())\n",
    "\n",
    "mcmc = MCMC(nuts_kernel, num_samples=2000, num_warmup=2000)\n",
    "rng_key = random.PRNGKey(0)\n",
    "mcmc.run(rng_key, highstreet_id, weeks, hs_obs=hs_obs)\n",
    "\n",
    "posterior_samples = mcmc.get_samples()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the model samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "dill_file = PROJECT_ROOT + \"/models/bayesian/posterior_samples_basic.pkl\"\n",
    "with open(dill_file, \"wb\") as f:\n",
    "    dill.dump(\n",
    "        posterior_samples,\n",
    "        f,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nuts_kernel_noncentered = NUTS(model_noncentered, init_strategy=init_to_feasible())\n",
    "\n",
    "mcmc_noncentered = MCMC(nuts_kernel_noncentered, num_samples=2000, num_warmup=2000)\n",
    "rng_key = random.PRNGKey(0)\n",
    "mcmc_noncentered.run(rng_key, highstreet_id, weeks, hs_obs=hs_obs)\n",
    "\n",
    "posterior_samples_noncentered = mcmc_noncentered.get_samples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dill_file = PROJECT_ROOT + \"/models/bayesian/posterior_samples_noncentered_basic.pkl\"\n",
    "with open(dill_file, \"wb\") as f:\n",
    "    dill.dump(\n",
    "        posterior_samples_noncentered,\n",
    "        f,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trace plot for centered model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = az.from_numpyro(mcmc)\n",
    "az.plot_trace(data, compact=True, figsize=(14, 32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trace plot for unnoncentered model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_noncentered = az.from_numpyro(mcmc_noncentered)\n",
    "az.plot_trace(data_noncentered, compact=True, figsize=(14, 32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine model output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcmc.print_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcmc_noncentered.print_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare predictions to data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "weeks_pred = train[train[\"highstreet_id\"] == 1][\"weeks_since_start\"]\n",
    "pred_template = []\n",
    "for i in np.unique(train[\"highstreet_id\"]):\n",
    "    df = pd.DataFrame(columns=[\"highstreet_id\", \"weeks\"])\n",
    "    df[\"weeks\"] = weeks_pred\n",
    "    df[\"highstreet_id\"] = i\n",
    "    pred_template.append(df)\n",
    "pred_template = pd.concat(pred_template, ignore_index=True)\n",
    "\n",
    "highstreet_id = pred_template[\"highstreet_id\"].values\n",
    "weeks = pred_template[\"weeks\"].values\n",
    "predictive = Predictive(model, posterior_samples, return_sites=[\"sigma\", \"obs\"])\n",
    "samples_predictive = predictive(random.PRNGKey(0), highstreet_id, weeks, None)\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    columns=[\"highstreet_id\", \"weeks_since_start\", \"txn_amt_pred\", \"sigma\"]\n",
    ")\n",
    "df[\"highstreet_id\"] = pred_template[\"highstreet_id\"]\n",
    "df[\"weeks_since_start\"] = pred_template[\"weeks\"]\n",
    "df[\"txn_amt_pred\"] = samples_predictive[\"obs\"].T.mean(axis=1)\n",
    "df[\"sigma\"] = samples_predictive[\"obs\"].T.std(axis=1)\n",
    "df[\"hs_inf\"] = df[\"txn_amt_pred\"] - df[\"sigma\"]\n",
    "df[\"hs_sup\"] = df[\"txn_amt_pred\"] + df[\"sigma\"]\n",
    "df = pd.merge(\n",
    "    df,\n",
    "    train[[\"highstreet_id\", \"weeks_since_start\", \"txn_amt\"]],\n",
    "    how=\"left\",\n",
    "    on=[\"highstreet_id\", \"weeks_since_start\"],\n",
    ")\n",
    "df = df.rename(columns={\"txn_amt\": \"txn_amt_true\"})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1261e040ca787f5ae3dfea195be8b65b79c591c00963204095cd345a6e2efeeb"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('highstreets')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
