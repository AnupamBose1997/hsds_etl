{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Science Accelerator - Predicting High Street Recovery\n",
    "\n",
    "The aim of this project is to predict high street recovery and identify high streets at risk, providing indicators of high street health which can be used to inform public policy and prioritise the most effective areas of intervention. Ths project aims to create a model which can be used to detect high streets with lower levels of recovery or those in decline. Data sources to be used include Mastercard spend data, provided weekly from 2018 until the present day. This data has been aggregated at a high street level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from functools import reduce\n",
    "\n",
    "import re\n",
    "import pickle\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import model_selection\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from highstreets.features import processing_functions as pf\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv(find_dotenv())\n",
    "\n",
    "HS_MCARD_TXN = os.environ.get(\"HS_MCARD_TXN\")\n",
    "LDN_MCARD_TXN = os.environ.get(\"LDN_MCARD_TXN\")\n",
    "O2_2021 = os.environ.get(\"O2_2021\")\n",
    "O2_2022 = os.environ.get(\"O2_2022\")\n",
    "HS_MSOA_LOOKUP = os.environ.get(\"HS_MSOA_LOOKUP\")\n",
    "PROFILE_FILE = os.environ.get(\"PROFILE_FILE\")\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in high streets transaction data\n",
    "mcard_highstreets = pd.read_csv(HS_MCARD_TXN)\n",
    "mcard_london = pd.read_csv(LDN_MCARD_TXN)\n",
    "\n",
    "# select weekday retail data\n",
    "hs = mcard_highstreets[\n",
    "    [\"yr\", \"wk\", \"week_start\", \"highstreet_id\", \"highstreet_name\", \"txn_amt_wd_retail\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in o2 footfall data\n",
    "o2_2021 = pd.read_csv(O2_2021, encoding=\"utf=8\")\n",
    "o2_2022 = pd.read_csv(O2_2022, encoding=\"utf-8\")\n",
    "# bind rows\n",
    "o2_concat = pd.concat([o2_2021, o2_2022])\n",
    "o2_cleaned = o2_concat[[\"msoa11cd\", \"count_date\", \"count_type\", \"h13\"]]\n",
    "# msoa - highstreet lookup\n",
    "hs_msoa_lookup = pd.read_csv(HS_MSOA_LOOKUP)\n",
    "lookup = hs_msoa_lookup[[\"msoa11cd\", \"highstreet_name\"]]\n",
    "# inner join dfs\n",
    "o2_hs = pd.merge(o2_cleaned, lookup, on=\"msoa11cd\")\n",
    "o2_hs = o2_hs[o2_hs[\"count_type\"] == \"Visitor\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in highstreet profile data\n",
    "hs_profiles = pd.read_excel(PROFILE_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data pre-processing\n",
    "\n",
    "Pre-processing of the Mastercard dataset is executed to ensure dates are in the correct format. Much of the pre-processing of the Mastercard data has already been performed in the processing scripts which load in the Mastercard data from the database. Feature scaling will also be applied as a pre-processing step prior to inputting the features into the algorithms, to account for the different sizes of the highstreets and the resulting large differences in their scale of spending patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MASTERCARD clean data and change to date by creating copy of existing list\n",
    "hs = hs[hs[\"week_start\"].notnull()].copy()\n",
    "hs[\"week_start\"] = pd.to_datetime(hs[\"week_start\"])\n",
    "\n",
    "# create month yr variable\n",
    "hs[\"month_year\"] = hs[\"week_start\"].dt.to_period(\"M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# O2 change date format\n",
    "o2_hs[\"count_date\"] = pd.to_datetime(o2_hs[\"count_date\"])\n",
    "# o2_hs['highstreet_name'] = o2_hs['highstreet_name'].encode('UTF-8')\n",
    "# group by vars\n",
    "o2_hs.groupby([\"count_date\", \"highstreet_name\", \"count_type\"]).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data labelling\n",
    "\n",
    "To feed the data into a supervised machine learning model, the data must have labels. Here, binary labels are created by categorising each high street as either at risk or not. These are categorised by looking at the averages of each x number of months, and if average spend is less than 75% of previous month's average spend, the high street will be categorised as 'at risk'. This creates the response dataset, the output variable that essentially depends on the feature variables. They are also known as target, label or output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelled = pf.create_labels(hs, \"2022-03\", \"2021-03\", \"2022-02\")\n",
    "labelled[\"labels\"] = labelled[\"labels\"].astype(\"int\")\n",
    "print(labelled.dtypes)\n",
    "# check class imbalance\n",
    "print(labelled.labels.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature engineering\n",
    "\n",
    "Calculate mean and standard deviation for each high street to transform the raw data into features. This created the feature dataset, also known as predictors, inputs or attributes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# O2 feature engineering\n",
    "day_range = pd.date_range(\"2021-05-08\", \"2022-04-01\", freq=\"d\")\n",
    "\n",
    "mean_sd_o2 = pf.create_mean_sd_o2(o2_hs, day_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create slice of march 2021 to march 2022\n",
    "months = pd.date_range(\"2021-03\", \"2022-03\", freq=\"m\").to_period(\"M\")\n",
    "mean_sd = pf.create_mean_sd_mcard(hs, months)\n",
    "\n",
    "# # create mean and sd for the two recovery periods\n",
    "# months_r1 = pd.date_range('2020-04', '2020-10', freq='m').to_period('M')\n",
    "# mean_sd_r1 = create_mean_sd_mcard(hs, months_r1)\n",
    "\n",
    "# months_r2 = pd.date_range('2021-01', '2021-04', freq='m').to_period('M')\n",
    "# mean_sd_r2 = create_mean_sd_mcard(hs, months_r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create a feature for the gradient, linear regression is used on each highstreet, with the coefficient taken as another feature to demonstrate the overall spending trend of each high street. The dates are converted from period objects to datetime objects and the into ordinal values, which can be inputted into scikitlearn. A question in creating this feature is whether to pool the transaction values by month, then average and find a trend line over that, or whether to leave the data as weekly values to find the gradient over this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "months = pd.date_range(\"2021-03\", \"2022-03\", freq=\"m\").to_period(\"M\")\n",
    "gradients = pf.create_gradient(hs, months)\n",
    "\n",
    "months_r1_grad = pd.date_range(\"2020-04\", \"2020-10\", freq=\"m\").to_period(\"M\")\n",
    "gradients_r1 = pf.create_gradient(hs, months_r1_grad)\n",
    "\n",
    "months_r2_grad = pd.date_range(\"2021-04\", \"2021-08\", freq=\"m\").to_period(\"M\")\n",
    "gradients_r2 = pf.create_gradient(hs, months_r2_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# O2 feature engineering pre methodology change\n",
    "day_range_pre = pd.date_range(\"2021-05-08\", \"2021-11-14\", freq=\"d\")\n",
    "gradient_o2_pre = pf.create_gradient_o2(o2_hs, day_range_pre)\n",
    "gradient_o2_pre = gradient_o2_pre.rename({\"gradient\": \"grad_o2_pre\"}, axis=1)\n",
    "# O2 post meth change\n",
    "day_range = pd.date_range(\"2021-11-15\", \"2022-04-01\", freq=\"d\")\n",
    "gradient_o2_post = pf.create_gradient_o2(o2_hs, day_range)\n",
    "gradient_o2_post = gradient_o2_post.rename({\"gradient\": \"grad_o2_post\"}, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge all the mastercard dataframes together into one feature df\n",
    "feature_list_mcard = [mean_sd, gradients, gradients_r1, gradients_r2]\n",
    "features_mcard = reduce(\n",
    "    lambda left, right: pd.merge(left, right, on=[\"highstreet_name\"], how=\"outer\"),\n",
    "    feature_list_mcard,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do same with o2\n",
    "features_o2 = mean_sd_o2.merge(gradient_o2_pre, on=\"highstreet_name\").merge(\n",
    "    gradient_o2_post, on=\"highstreet_name\"\n",
    ")\n",
    "# quick fix, needs work asap\n",
    "# as encoding of o2 files is wrong, will change hs name apostrophes\n",
    "col = [[re.sub(r\"[^\\x00-\\x7f]\", \"'\", i)] for i in mean_sd_o2[\"highstreet_name\"]]\n",
    "col = sum(col, [])\n",
    "\n",
    "features_o2.drop([\"highstreet_name\"], axis=1)\n",
    "features_o2[\"highstreet_name\"] = col\n",
    "\n",
    "feature_complete = features_mcard.merge(features_o2, on=\"highstreet_name\")\n",
    "feature_complete.drop([\"highstreet_name\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split into test and training sets\n",
    "\n",
    "The data is split into training and test sets, with a 70/30 split. Normalisation is then applied to both test and train data, though seperately to avoid including information from the future validation set into the training data. . Feature normalization (or data standardization) of the explanatory (or predictor) variables is a technique used to center and normalise the data by subtracting the mean and dividing by the variance. If you take the mean and variance of the whole dataset you'll be introducing future information into the training explanatory variables (i.e. the mean and variance). Therefore, you should perform feature normalisation over the training data. Then perform normalisation on testing instances as well, but this time using the mean and variance of training explanatory variables. In this way, we can test and evaluate whether our model can generalize well to new, unseen data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix error about data format, wants an array\n",
    "labelled = np.ravel(labelled)\n",
    "\n",
    "# testing with different\n",
    "# split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    feature_complete, labelled, test_size=0.3, random_state=0\n",
    ")\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)\n",
    "\n",
    "# normalise\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross validation and hyperparameter tuning\n",
    "\n",
    "The models are cross validated using the training data, after which the optimal model is chosen to use on the test data for the final model evaluation. The base model using only the mastercard spend gradient as a feature had an accuracy score of 0.33 for SVM and XGBoost. Adding in the mean and sd, log regression accuracy is now 0.48 and xgboost at 0.419. Including mean_sd, and gradients of time period and two recovery periods = random forest 0.48, xg and log 0.465."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters to test for each algorithm\n",
    "log_reg_params = [{\"C\": 0.01}, {\"C\": 0.1}, {\"C\": 1}, {\"C\": 10}]\n",
    "dec_tree_params = [{\"criterion\": \"gini\"}, {\"criterion\": \"entropy\"}]\n",
    "rand_for_params = [{\"criterion\": \"gini\"}, {\"criterion\": \"entropy\"}]\n",
    "kneighbors_params = [{\"n_neighbors\": 3}, {\"n_neighbors\": 5}]\n",
    "naive_bayes_params = [{}]\n",
    "svc_params = [{\"C\": 0.01}, {\"C\": 0.1}, {\"C\": 1}, {\"C\": 10}]\n",
    "xgb_params = [{}]\n",
    "\n",
    "# list of models, params etc\n",
    "modelclasses = [\n",
    "    [\"log regression\", LogisticRegression, log_reg_params],\n",
    "    [\"decision tree\", DecisionTreeClassifier, dec_tree_params],\n",
    "    [\"random forest\", RandomForestClassifier, rand_for_params],\n",
    "    [\"k neighbors\", KNeighborsClassifier, kneighbors_params],\n",
    "    [\"naive bayes\", GaussianNB, naive_bayes_params],\n",
    "    [\"support vector machines\", SVC, svc_params],\n",
    "    [\"xgboost\", XGBClassifier, xgb_params],\n",
    "]\n",
    "\n",
    "# loop through each model with k-fold cv\n",
    "insights = []\n",
    "for modelname, Model, params_list in modelclasses:\n",
    "    for params in params_list:\n",
    "        model = Model(**params)\n",
    "        kfold = model_selection.KFold(n_splits=10, random_state=None)\n",
    "        cv_results = model_selection.cross_val_score(\n",
    "            model, X_train, y_train, cv=kfold, scoring=\"accuracy\"\n",
    "        )\n",
    "        mean = cv_results.mean()\n",
    "        sd = cv_results.std()\n",
    "        insights.append((modelname, model, params, cv_results, mean, sd))\n",
    "\n",
    "insights.sort(key=lambda x: x[-2], reverse=True)\n",
    "with open(\"models.pickle\", \"wb\") as f:\n",
    "    for modelname, model, params, cv_results, mean, sd in insights:\n",
    "        print(modelname, params, \"Accuracy:\", round(mean, 3))\n",
    "        # pickle models\n",
    "        pickle.dump(model, f)\n",
    "\n",
    "results = pd.DataFrame(\n",
    "    insights, columns=[\"model name\", \"model\", \"parameter\", \"accuracy\", \"mean\", \"s.d.\"]\n",
    ")\n",
    "\n",
    "# boxplot algorithm comparison\n",
    "fig = plt.figure()\n",
    "fig.suptitle(\"Algorithm Comparison\")\n",
    "ax = fig.add_subplot(111)\n",
    "plt.boxplot(results[\"accuracy\"])\n",
    "ax.set_xticklabels(results[\"model name\"])\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load models back into memory\n",
    "models = []\n",
    "with open(\"models.pickle\", \"rb\") as f:\n",
    "    while True:\n",
    "        try:\n",
    "            models.append(pickle.load(f))\n",
    "        except EOFError:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Final model evaluation after hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(C=10, max_iter=150)\n",
    "# model = RandomForestClassifier()\n",
    "# model = XGBClassifier()\n",
    "# fit classifier and flatten arrays\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "# Finding accuracy by comparing actual response values (y_test)\n",
    "# with predicted response value (y_pred)\n",
    "print(\"Accuracy:\", metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"F1:\", metrics.f1_score(y_test, y_pred, average=\"weighted\"))\n",
    "# confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "disp = ConfusionMatrixDisplay(cm)\n",
    "disp.plot()\n",
    "plt.show()\n",
    "# accuracy of multiclass classification problem\n",
    "precision, recall, fscore, support = score(y_test, y_pred)\n",
    "\n",
    "FP = cm.sum(axis=0) - np.diag(cm)\n",
    "FN = cm.sum(axis=1) - np.diag(cm)\n",
    "TP = np.diag(cm)\n",
    "TN = cm.sum() - (FP + FN + TP)\n",
    "FP = FP.astype(float)\n",
    "FN = FN.astype(float)\n",
    "TP = TP.astype(float)\n",
    "TN = TN.astype(float)\n",
    "\n",
    "acc = (TP + TN) / (TP + FP + FN + TN)\n",
    "print(acc)\n",
    "\n",
    "print(\"Class accuracy:\", cm.diagonal() / cm.sum(axis=0))\n",
    "print(\"Precision: {}\".format(precision))\n",
    "print(\"Recall: {}\".format(recall))\n",
    "print(\"F-score: {}\".format(fscore))\n",
    "print(\"Support: {}\".format(support))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fbff9bfab3ee0df4124016b4e010029cf2ec5864ce5e3c4aa09796cc364af95f"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
